---
title: "Demand needs purchasing power (AGI economics)"
date: 2026-02-03
type: note
topics:
  - AGI
tags: []
---

# Demand needs purchasing power (AGI economics)

Source (X): https://x.com/i/status/2016317410853220827
Author: @timoreilly
Date: 2026-02-03

## Content (verbatim)

Rana Foroohar started out her Swamp Notes column in the Financial Times this week with a provocation about the already enshittified experience of too many AI agents coupled with the issue of the falling labor share of the economy. She wondered how to square the circle, and asked me my thoughts. Here's what I had to say:

"While I feel a lot of sympathy for your frustration with bots that don’t get it quite right, that isn’t the heart of the problem. The bots will get better. The problem is what happens when they do. The narrative from the AI labs is that when they build artificial general intelligence (AGI), it will unlock astonishing productivity and GDP will surge. It sounds compelling, especially if you’re the one building or investing in AI. But an economy isn’t just production. It is production matched to demand, and demand requires broadly distributed purchasing power. You can’t build a prosperous society that leaves most people on the sidelines.

We are told that businesses will become more efficient as AI substitutes for intellectual work, much as it did when machines replaced animals and human manual labour. But who are the customers when a large number of humans are suddenly no longer gainfully employed? This is not a rhetorical question. It is a constraint that Silicon Valley prefers not to model. You can’t rapidly replace wages with inference and expect the consumer economy to hum along unchanged. If the wage share falls fast enough, the economy may become less stable. The risk of social conflict and political backlash rises. Hopes that new jobs will be invented and vague hand-waving at Universal Basic Income funded by the generosity of fabulously wealthy AI companies is not a winning strategy.

In a lot of ways, corporate governance has been a trial run of AI governance, and we failed the test. Oxford philosopher Nick Bostrom’s famous parable of misaligned incentives features an AI-controlled factory making paper clips; the AI decides it can achieve its prime objective more effectively by eliminating humans, forgetting that it is humans who wanted the paper clips in the first place. That’s exactly what companies do when they use AI to eliminate human workers so the productivity gains can flow to shareholders. We’ve built an economic operating system increasingly optimised for a single goal: maximise returns to capital.

This isn’t inevitable. Henry Ford understood that mass markets need mass purchasing power. In the early days of the internet, Google and Amazon understood circulation too. They built flywheels that served customers and suppliers together.

If the AI labs want to be architects of a prosperous future, they have to do the same thing. They have to work as hard on inventing the new economy’s circulatory system as they do on improving model capabilities.

I love the line in Neal Stephenson’s novel Reamde where the main character describes the design of the economic system of the game that made him the world’s richest man: “You had to build the plumbing first: you had to get the whole money flow system worked out. Once that was done, everything else would follow.”

I see the current AI economic model as a kind of colonialism: The AI labs have taken the world’s intellectual output without compensation, transformed it and made it valuable for their customers. What happens to the companies and jobs they displace is left for “the market” to magically come up with on its own. The money flow they envision is mostly one way: money flowing to the AI industry from investors, and then from customers consuming their AI tokens. Most of them don’t spend enough time imagining and supporting their customers in making money too. They’ve built a hub and spoke system of value capture and control, not an exchange economy. (Maybe a little too geeky for you here, but I do see green shoots of a more participatory AI economy in Anthropic’s Model Context Protocol and Claude Skills.)

If AI is really going to grow GDP, the productivity dividend should show up for companies in growth and profits as they build new markets, not just as they extract a bit more on the way down. And it should show up for employees through some combination of higher pay, reduced hours, profit-sharing, and investment in retraining, not as a pink slip!

But it’s not going to be easy to get there. As a result, governments should be developing scenarios for a future in which taxes on labour might provide a much smaller part of their income. Solutions aren’t obvious, and transitions will be hard, but if we face a future where capital appreciation is abundant and labour income is scarce, perhaps it’s time to consider reducing taxes on labour and increasing those on capital gains.

“Slow down AI” isn’t the answer. The genie is out of the bottle. The answer has to be building the missing half of the AI economy.
"